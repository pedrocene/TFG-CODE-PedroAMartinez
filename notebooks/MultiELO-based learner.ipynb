{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import math\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfolder = '/Users/pedroantonio/Desktop/ELO/Datos Shadowspect'\n",
    "datafile = 'datos_ELO.csv'\n",
    "trainfile = 'datos_ELO_train.csv'\n",
    "testfile = 'datos_ELO_test.csv'\n",
    "\n",
    "#datafile = 'prueba.csv'\n",
    "#trainfile = 'pruebaTrain.csv'\n",
    "#testfile = 'pruebaTest.csv'\n",
    "\n",
    "student_id = 'user'\n",
    "student_column_number = 1\n",
    "group_column_number = 0\n",
    "completed = 'n_completed'\n",
    "puzzle_name = 'task_id'\n",
    "puzzle_column_number = 2\n",
    "kc_column = 'kc'\n",
    "kc_column_number = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg1Puzzles = ['Bird Fez', 'Pi Henge', 'Bull Market']\n",
    "gmd4Puzzles = ['Angled Silhouettes', 'Not Bird', 'Stranger Shapes', 'Ramp Up and Can It', 'Few Clues']\n",
    "co5Puzzles = ['45-Degree Rotations', 'Boxes Obscure Spheres', 'More Than Meets the Eye']\n",
    "co6Puzzles = ['Tall and Small', 'Not Bird', 'Ramp Up and Can It', 'Stretch a Ramp', 'Max 2 Boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readInputFile(PathName, FileName):\n",
    "    if not os.path.exists(PathName):\n",
    "        os.makedirs(PathName)\n",
    "    \n",
    "    FilePath = PathName + \"/\" + FileName\n",
    "    total_data = pd.read_csv(FilePath)\n",
    "    \n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Diccionario con los ids de usuario y número: uDict\n",
    "def usersDict(pathName, fileName):\n",
    "    ufile = pathName + \"/\" + fileName\n",
    "    csv_file = csv.reader(open(ufile, \"r\"), delimiter= str(','))\n",
    "    next(csv_file)\n",
    "    mapUsers = {}\n",
    "    mapGroups = {}\n",
    "    cont =0\n",
    "    for row in csv_file:\n",
    "        user = row[student_column_number]\n",
    "        group = row[group_column_number]\n",
    "        if user not in mapUsers:\n",
    "            mapUsers[user]=cont\n",
    "            mapGroups[user] = group\n",
    "            cont = cont+1\n",
    "    return mapUsers, mapGroups  \n",
    "\n",
    "\n",
    "# Diccionario en el que el nombre del paso se asigna como un nombre de pregunta distinto al diccionario: qDict\n",
    "def puzzlesDict(pathName, fileName):\n",
    "    qfile = pathName + \"/\" + fileName\n",
    "    csv_file = csv.reader(open(qfile, \"r\"), delimiter= str(','))\n",
    "    next(csv_file)\n",
    "    mapPuzzles = {}\n",
    "    cont =0\n",
    "    for row in csv_file:\n",
    "        question = row[puzzle_column_number]\n",
    "        if question not in mapPuzzles:\n",
    "            mapPuzzles[question]=cont\n",
    "            cont = cont+1\n",
    "    return mapPuzzles\n",
    "\n",
    "\n",
    "\n",
    "# Diccionario en el que el KC se mapea como etiquetas del diccionario: kcDict \n",
    "def kcsDict(pathName, fileName):\n",
    "    qfile = pathName + \"/\" + fileName\n",
    "    QT = []\n",
    "    csv_file = csv.reader(open(qfile, \"r\"), delimiter= str(','))\n",
    "    next(csv_file)\n",
    "    mapKc = {}\n",
    "    cont =0\n",
    "    for row in csv_file:\n",
    "        tags = row[kc_column_number] \n",
    "        if tags:\n",
    "            tag = tags.split(\"~\")\n",
    "            for topics in tag:\n",
    "                if topics not in mapKc:\n",
    "                    mapKc[topics]=cont\n",
    "                    cont = cont + 1\n",
    "    return mapKc\n",
    "\n",
    "def createKcDict(pathName, fileName):\n",
    "    \n",
    "    QTMat = dict()\n",
    "    qfile = pathName + \"/\" + fileName\n",
    "    csv_file = csv.reader(open(qfile, \"r\"), delimiter=\",\")\n",
    "    next(csv_file)\n",
    "    #cont=0\n",
    "    for row in csv_file:\n",
    "        qid = row[puzzle_column_number]\n",
    "        kcs = row[kc_column_number]\n",
    "        if(qid not in QTMat.keys()):\n",
    "            #questions[cont]=qid\n",
    "            QTMat[qid]=dict()\n",
    "            #cont=cont+1\n",
    "        if kcs:\n",
    "            kc = kcs.split(\"~\")\n",
    "            for k in kc:\n",
    "                #if qid in qDict and k in tDict:\n",
    "                QTMat[qid][k] =0\n",
    "\n",
    "\n",
    "    for puzzle in QTMat.keys():\n",
    "        tam = len(QTMat[puzzle])\n",
    "        #Se comprueba que tenga al menos un kc\n",
    "        if tam>0:   \n",
    "            if(puzzle in mg1Puzzles):\n",
    "                QTMat[puzzle]['MG.1'] = 0.5\n",
    "                for x in QTMat[puzzle].keys():\n",
    "                    if(x != 'MG.1'):\n",
    "                        QTMat[puzzle][x] = 0.5/(tam-1)\n",
    "            elif(puzzle in gmd4Puzzles): \n",
    "                QTMat[puzzle]['GMD.4'] = 0.5\n",
    "                for x in QTMat[puzzle].keys():\n",
    "                    if(x != 'GMD.4'):\n",
    "                        QTMat[puzzle][x] = 0.5/(tam-1)\n",
    "            elif(puzzle in co5Puzzles): \n",
    "                QTMat[puzzle]['CO.5'] = 0.5\n",
    "                for x in QTMat[puzzle].keys():\n",
    "                    if(x != 'CO.5'):\n",
    "                        QTMat[puzzle][x] = 0.5/(tam-1)\n",
    "            elif(puzzle in co6Puzzles):  \n",
    "                QTMat[puzzle]['CO.6'] = 0.5\n",
    "                for x in QTMat[puzzle].keys():\n",
    "                    if(x != 'CO.6'):\n",
    "                        QTMat[puzzle][x] = 0.5/(tam-1)              \n",
    "            else:\n",
    "                #Se dividen a partes iguales los kc\n",
    "                for x in QTMat[puzzle].keys():\n",
    "                    QTMat[puzzle][x] = 1/tam\n",
    "    return QTMat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(inputfolder):\n",
    "    uDict, gDict = usersDict(inputfolder, datafile) \n",
    "    qDict =puzzlesDict(inputfolder, datafile)\n",
    "    kcDict =kcsDict(inputfolder, datafile)\n",
    "    kcsPuzzleDict =  createKcDict(inputfolder, datafile) \n",
    "\n",
    "    return uDict, gDict,qDict,kcDict, kcsPuzzleDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtener un valor RMSE basado en las predicciones del modelo y las respuestas reales\n",
    "def rmseFunction(prob, ans, lenProb):\n",
    "    prob = np.array(prob)\n",
    "    ground = np.array(ans)\n",
    "    error = (prob - ans) \n",
    "    err_sqr = error*error\n",
    "    rmse = math.sqrt(err_sqr.sum()/lenProb)\n",
    "    return rmse  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Obtener un valor de accuracy basado en las predicciones de los modelos y las respuestas reales\n",
    "def accuracyFunction(ans, prob): \n",
    "    ans = np.array(ans)\n",
    "    prob = np.array(prob)\n",
    "    prob[prob >= 0.5] = 1\n",
    "    prob[prob < 0.5] = 0\n",
    "    acc = metrics.accuracy_score(ans, prob)\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiTopic_ELO(inputData, Competency, Diff,groupDiff, A_count, Q_count, kcsPuzzleDict ,gDict,gamma, beta): \n",
    "\n",
    "    alpha = 1\n",
    "    alpha_denominator = 0\n",
    "    correct = 0\n",
    "    prob_test = dict()\n",
    "    ans_test = dict()  \n",
    "\n",
    "    response = np.zeros((len(inputData), 1))\n",
    "    \n",
    "    for count, (index, item) in enumerate(inputData.iterrows()):\n",
    "        alpha_denominator = 0\n",
    "        uid = item[student_id] \n",
    "        qid = item[puzzle_name] \n",
    "        diff = Diff[qid] \n",
    "        comp= dict()\n",
    "        comp[uid]=[]\n",
    "        for k in kcsPuzzleDict[qid]:\n",
    "            comp[uid].append(Competency[uid][k] * kcsPuzzleDict[qid][k])\n",
    "        compTotal = np.sum(comp[uid])\n",
    "        probability = (1)/(1 + math.exp( -1 * (compTotal - diff)))\n",
    "        if(uid not in prob_test.keys()):\n",
    "            prob_test[uid] = dict()\n",
    "        prob_test[uid][qid]=probability\n",
    "        q_answered_count = Q_count[qid] \n",
    "        \n",
    "        if item[completed] == 1:\n",
    "\n",
    "            response[count] = 1\n",
    "            correct = 1\n",
    "        else:\n",
    "            response[count] = 0\n",
    "            correct = 0\n",
    "        \n",
    "        #Se almacena la respuesta    \n",
    "        if(uid not in ans_test.keys()):\n",
    "            ans_test[uid] = dict()\n",
    "        ans_test[uid][qid] = correct \n",
    "        \n",
    "        groupDiff[gDict[uid]][qid] = groupDiff[gDict[uid]][qid] + ((gamma)/(1 + beta * q_answered_count)) * (probability - correct)\n",
    "        \n",
    "        Diff[qid] = Diff[qid] + ((gamma)/(1 + beta * q_answered_count)) * (probability - correct)\n",
    "        Q_count[qid] += 1\n",
    "        \n",
    "        #Se calcula alpha\n",
    "        alpha_numerator = probability - correct\n",
    "        for k in kcsPuzzleDict[qid]:\n",
    "            #if(T[qid][k] != 0): \n",
    "            c_lambda = Competency[uid][k]\n",
    "            probability_lambda = (1)/(1 + math.exp( -1 * (c_lambda - diff)))\n",
    "            alpha_denominator = alpha_denominator + (correct - probability_lambda)\n",
    "        alpha = abs(alpha_numerator / alpha_denominator)\n",
    "\n",
    "        #Actualizando el nivel de competencia del estudiante en cada pregunta con la que la pregunta está etiquetada \n",
    "        for k in kcsPuzzleDict[qid]:\n",
    "            #if(T[qid][k] != 0):\n",
    "            u_answered_count = A_count[uid][k]\n",
    "            c = Competency[uid][k] \n",
    "            probability = (1)/(1 + math.exp( -1 * (compTotal - diff)))\n",
    "            \n",
    "            Competency[uid][k] = Competency[uid][k]+kcsPuzzleDict[qid][k] * (gamma)/(1 + beta * u_answered_count) * alpha * (correct - probability)\n",
    "            #Competency[uid][k] = Competency[uid][k]+ (gamma)/(1 + beta * u_answered_count) * alpha * (correct - probability)\n",
    "            #print(\"Pregunta: \", qid)\n",
    "            #print(\"Competency[uid][k]\",uid,\"-\",k, Competency[uid][k])\n",
    "            #print(\"kcsPuzzleDict[qid][k]\",uid,\"-\",k, kcsPuzzleDict[qid][k])\n",
    "            A_count[uid][k] += 1\n",
    "                \n",
    "    return Competency, Diff,groupDiff, A_count , Q_count, prob_test, ans_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runExperiment(model, inputfolder, gamma, beta):\n",
    "    #Se cargan los datos y se rellenan las estructuras de datos\n",
    "    uDict,gDict,qDict,kcDict,kcsPuzzleDict = loadDataset(inputfolder)\n",
    "\n",
    "    #Se leen los archivos separados en train y test\n",
    "    train_set = readInputFile(inputfolder, trainfile)\n",
    "    test_set = readInputFile(inputfolder, testfile)\n",
    "\n",
    "    if model == 'multiTopic':\n",
    "        \n",
    "        group_difficulty = dict()\n",
    "        question_difficulty = dict() \n",
    "        question_counter = dict() \n",
    "        for g in gDict.values():\n",
    "            group_difficulty[g] = dict()\n",
    "            for q in qDict.keys():\n",
    "                question_difficulty[q]=0\n",
    "                question_counter[q]=0\n",
    "                group_difficulty[g][q]=0\n",
    "        \n",
    "        \n",
    "        learner_competency = dict()  \n",
    "        response_counter = dict() \n",
    "        for user in uDict.keys():\n",
    "            if(user not in learner_competency.keys()):\n",
    "                learner_competency[user]=dict()\n",
    "                response_counter[user]=dict()\n",
    "            for k in kcDict.keys():\n",
    "                learner_competency[user][k]=0\n",
    "                response_counter[user][k]=0\n",
    "\n",
    "\n",
    "        learner_competency_train, question_difficulty_train,group_difficulty_train, response_counter_train, question_counter_train, prob_train, ans_train   = multiTopic_ELO(train_set, learner_competency, question_difficulty,group_difficulty, response_counter, question_counter, kcsPuzzleDict,gDict,gamma, beta)\n",
    "        learner_competency_test, question_difficulty_test,group_difficulty_test, response_counter_test, question_counter_test, prob_test, ans_test   = multiTopic_ELO(test_set, learner_competency_train, question_difficulty_train,group_difficulty_train, response_counter_train, question_counter_train, kcsPuzzleDict,gDict,gamma, beta)\n",
    "\n",
    "    \n",
    "    # tamaño 300\n",
    "    #print(\"Matriz: \")\n",
    "    #print(kcsPuzzleDict)\n",
    "    #print(\"Competencia de aprendizaje: \")\n",
    "    #print(learner_competency)\n",
    "    \n",
    "    #print(\"Dificultad de cuestiones: \")\n",
    "    #print(question_difficulty)\n",
    "    \n",
    "    #print(\"Contador de respuestas de cada alumno: \")\n",
    "    #print(response_counter)\n",
    "    \n",
    "    #print(\"Contador de respuestas a cada pregunta: \")\n",
    "    #print(question_counter)\n",
    "    \n",
    "    #print(\"prob_train: \", prob_train)\n",
    "    #print(\"p_test: \", p_test)\n",
    "    #print(\"a_test: \", a_test)\n",
    "    \n",
    "    ######### Normalización competency ###########\n",
    "    totalCompetencyGMD = []\n",
    "    totalCompetencyCO5 = []\n",
    "    totalCompetencyCO6 = []\n",
    "    totalCompetencyMG1 = []\n",
    "    \n",
    "    for user in learner_competency.keys():\n",
    "        for x in learner_competency[user]:\n",
    "            if(x == 'GMD.4'):\n",
    "                totalCompetencyGMD.append(learner_competency[user][x])\n",
    "            elif(x == 'CO.5'):\n",
    "                totalCompetencyCO5.append(learner_competency[user][x]) \n",
    "            elif(x == 'CO.6'):\n",
    "                totalCompetencyCO6.append(learner_competency[user][x])\n",
    "            elif(x == 'MG.1'):\n",
    "                totalCompetencyMG1.append(learner_competency[user][x])    \n",
    "            \n",
    "    minCompetencyGMD = min(totalCompetencyGMD)   \n",
    "    maxCompetencyGMD = max(totalCompetencyGMD)\n",
    "    \n",
    "    minCompetencyCO5 = min(totalCompetencyCO5)   \n",
    "    maxCompetencyCO5 = max(totalCompetencyCO5)\n",
    "    \n",
    "    minCompetencyCO6 = min(totalCompetencyCO6)   \n",
    "    maxCompetencyCO6 = max(totalCompetencyCO6)\n",
    "    \n",
    "    minCompetencyMG1 = min(totalCompetencyMG1)   \n",
    "    maxCompetencyMG1 = max(totalCompetencyMG1)\n",
    "    \n",
    "    normalized_learner_competency = dict()\n",
    "    for user in learner_competency.keys():\n",
    "        normalized_learner_competency[user]=dict()\n",
    "        for x in learner_competency[user]:\n",
    "            if(x == 'GMD.4'):\n",
    "                normalized_learner_competency[user][x]= (learner_competency[user][x]- minCompetencyGMD)/(maxCompetencyGMD-minCompetencyGMD)\n",
    "            elif(x == 'CO.5'):\n",
    "                normalized_learner_competency[user][x]= (learner_competency[user][x]- minCompetencyCO5)/(maxCompetencyCO5-minCompetencyCO5) \n",
    "            elif(x == 'CO.6'):\n",
    "                normalized_learner_competency[user][x]= (learner_competency[user][x]- minCompetencyCO6)/(maxCompetencyCO6-minCompetencyCO6)\n",
    "            elif(x == 'MG.1'):\n",
    "                normalized_learner_competency[user][x]= (learner_competency[user][x]- minCompetencyMG1)/(maxCompetencyMG1-minCompetencyMG1)\n",
    "            \n",
    "    \n",
    "    #print(group_difficulty)\n",
    "    ######## Normalización difficulty ###########\n",
    "    \n",
    "    normalized_question_difficulty = dict()\n",
    "    for puzzle in question_difficulty.keys():\n",
    "        if(puzzle not in question_difficulty.keys()):\n",
    "            normalized_question_difficulty[puzzle] = 0\n",
    "        normalized_question_difficulty[puzzle] = (question_difficulty[puzzle]-min(question_difficulty.values()))/(max(question_difficulty.values())-min(question_difficulty.values()))\n",
    "        \n",
    "    ######## Normalización group difficulty ###########\n",
    "    normalized_group_difficulty = dict()\n",
    "    for group in group_difficulty.keys():\n",
    "        normalized_group_difficulty[group] = dict()\n",
    "        for puzzle in group_difficulty[group].keys():\n",
    "            normalized_group_difficulty[group][puzzle]=0\n",
    "            normalized_group_difficulty[group][puzzle] = (group_difficulty[group][puzzle]-min(group_difficulty[group].values()))/(max(group_difficulty[group].values())-min(group_difficulty[group].values()))\n",
    "       \n",
    "    \n",
    "    #print(normalized_group_difficulty)\n",
    "    group_prob_test = []\n",
    "    for user in prob_test.keys():\n",
    "        for task in prob_test[user].keys():\n",
    "            group_prob_test.append(prob_test[user][task])\n",
    "            \n",
    "    group_ans_test = []\n",
    "    for user in ans_test.keys():\n",
    "        for task in ans_test[user].keys():\n",
    "            group_ans_test.append(ans_test[user][task])        \n",
    "                   \n",
    "    rmse = rmseFunction(group_prob_test, group_ans_test, len(group_prob_test))\n",
    "    auc = aucFunction(group_ans_test, group_prob_test)\n",
    "    accuracy = accuracyFunction(group_ans_test, group_prob_test)\n",
    "    kappa = cohenKappaFunction(group_ans_test, group_prob_test)\n",
    "\n",
    "    return rmse, auc, accuracy, kappa \n",
    "    #return normalized_group_difficulty\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "RMSE:  0.2997796848376026\n",
      "Accuracy:  0.8868327402135231\n",
      " \n"
     ]
    }
   ],
   "source": [
    "rmse_multiTopic, auc_multiTopic, acc_multiTopic, kappa_multiTopic = runExperiment('multiTopic', inputfolder, 1.8, 0.05)\n",
    "print(\" \")\n",
    "print(\"RMSE: \", rmse_multiTopic)\n",
    "#print(\"AUC: \",auc_multiTopic)\n",
    "print(\"Accuracy: \", acc_multiTopic)\n",
    "#print(\"Kappa: \",kappa_multiTopic)\n",
    "print(\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datos = runExperiment('multiTopic', inputfolder, 1.8, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/pedroantonio/Desktop/ELO/datosCompetencyELO_normalized.csv\", 'w', newline='') as csvfile:\n",
    "    fieldnames = ['group', 'user', 'kc', 'competency']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for user in datos:\n",
    "        for x in datos[user]:\n",
    "            writer.writerow({'group': grupos[user],'user': user, 'kc': x, 'competency': datos[user][x] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/pedroantonio/Desktop/ELO/difficultyELO-Multi.csv\", 'w', newline='') as csvfile:\n",
    "    fieldnames = ['task_id', 'difficulty']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for key in datos:\n",
    "        writer.writerow({'task_id': key, 'difficulty': datos[key]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/pedroantonio/Desktop/ELO/probabilityTest_ELO-Multi.csv\", 'w', newline='') as csvfile:\n",
    "    fieldnames = ['user', 'task_id', 'probability']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for user in datos:\n",
    "        for x in datos[user]:\n",
    "            writer.writerow({'user': user, 'task_id': x, 'probability': datos[user][x] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/pedroantonio/Desktop/ELO/datosDifficultyELO_normalized.csv\", 'w', newline='') as csvfile:\n",
    "    fieldnames = ['group', 'task_id', 'difficulty']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for group in datos:\n",
    "        for puzzle in datos[group]:\n",
    "            writer.writerow({'group': group, 'task_id': puzzle, 'difficulty': datos[group][puzzle] })\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
